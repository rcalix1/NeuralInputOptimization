# -*- coding: utf-8 -*-
"""FP520NIO Regression

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xP4QFVvl102leD4LNoJM3_NGC2dKH7Nn
"""

# This ensures Colab has all required packages for training the model.
# Most are preinstalled, but we install joblib to be safe.

#!pip install joblib scikit-learn torch
# Import all required libraries for:
# - data loading
# - model creation
# - model training
# - metrics calculation
# - saving artifacts

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split

import pandas as pd
import numpy as np
import json
import joblib
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score

import os

CSV_PATH = 'Concrete_Compressive_StrengthV3.csv'

# Load the concrete dataset from the file you uploaded.
# This data will be used to train the model from scratch.
# *Strength feature is Concrete Compressive Strength(Very Important)*
# *Age feature is measured in Days(Very Important)*

df = pd.read_csv(CSV_PATH)
df.head()

# Separate input features (X) and target output (y).
# The target column is "Strength", and all other columns are features.

feature_cols = [c for c in df.columns if c.lower() != "strength"]
target_col = "Strength"

# Convert to NumPy arrays with correct dtype for PyTorch

X = df[feature_cols].values.astype(np.float32)
y = df[target_col].values.astype(np.float32).reshape(-1, 1)

X[:5], y[:5]  # Preview data

# Scale all features into the [0, 1] range using MinMaxScaler.
# This scaler MUST be saved so deployment can use the same scaling.

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Save the scaler to disk so FastAPI can use it!
joblib.dump(scaler, "scaler.pkl")
print("Saved scaler.pkl")

# Convert the scaled NumPy arrays into a PyTorch Dataset class.
# This allows batching, shuffling, and splitting for training.

class ConcreteDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        # Returns one example (features, target)
        return self.X[idx], self.y[idx]

# Create the dataset object
dataset = ConcreteDataset(X_scaled, y)

# 80/20 train-validation split
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size

train_ds, val_ds = random_split(dataset, [train_size, val_size])

# DataLoaders provide batches to the model during training
train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=32)

len(train_loader), len(val_loader)

# This is the neural network architecture.

class ConcreteRegressor(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        return self.net(x)

# Initialize the model using the number of columns in the dataset.
# I also define MSE as the loss function, and Adam as the optimizer.

model = ConcreteRegressor(input_dim=X_scaled.shape[1])
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
epochs = 500  # Training duration

# This loop:
# - Trains the model on your concrete data
# - Computes validation loss each epoch
# - Computes RÂ² score (important for regression)
# - Shows progress for each epoch

best_r2 = -999  # Track best R2

for epoch in range(epochs):
    model.train()
    train_loss = 0

    # Train on all batches
    for Xb, yb in train_loader:
        optimizer.zero_grad()
        pred = model(Xb)
        loss = criterion(pred, yb)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

    # Validation
    model.eval()
    val_loss = 0
    preds, targets = [], []

    with torch.no_grad():
        for Xb, yb in val_loader:
            pred = model(Xb)
            loss = criterion(pred, yb)
            val_loss += loss.item()
            preds.append(pred.numpy())
            targets.append(yb.numpy())

    preds = np.vstack(preds)
    targets = np.vstack(targets)
    r2 = r2_score(targets, preds)

    # Display training progress
    print(f"Epoch {epoch+1}/{epochs} | "
          f"Train Loss: {train_loss/len(train_loader):.4f} | "
          f"Val Loss: {val_loss/len(val_loader):.4f} | "
          f"R2: {r2:.4f}")

# Save the model weights so they can be loaded later in FastAPI or NIO.
# This file contains ONLY the trained weights.

torch.save(model.state_dict(), "model_v1.pt")
print("Saved model_v1.pt")

# Save information the deployment system must know:
# - order of input features
# - input dimension
# - target column name
# - which scaler is used

metadata = {
    "feature_order": feature_cols,
    "target_name": target_col,
    "input_dim": len(feature_cols),
    "scaled": True,
    "scaler_type": "MinMaxScaler"
}

with open("metadata.json", "w") as f:
    json.dump(metadata, f, indent=4)

print("Saved metadata.json")

# Save training metrics so your partner can display them in FastAPI.
# These values summarize the quality of your model.

final_metrics = {
    "validation_mse": float(val_loss / len(val_loader)),
    "validation_r2": float(r2),
    "epochs": epochs
}

with open("metrics.json", "w") as f:
    json.dump(final_metrics, f, indent=4)

print("Saved metrics.json")
print(final_metrics)
