# -*- coding: utf-8 -*-
"""FP520NIO NIO

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uD_cE0ZyxAshpWDfp3_pFiIMrdBjg8IB
"""

# Installs any Colab missing dependencies.
!pip install joblib scikit-learn torch
# Import all dependencies needed for:
# - loading trained model & artifacts
# - preparing inputs
# - performing gradient-based optimization (NIO)

import torch
import torch.nn as nn
import torch.optim as optim
import json
import joblib
import numpy as np
from sklearn.preprocessing import MinMaxScaler

# This cell loads the model weights, scaler, and metadata
# that were created during training.
# Mount Google Drive
from google.colab import drive
import os

drive.mount('/content/drive')

MODEL_PATH = '/content/drive/My Drive/model_v1.pt'
SCALER_PATH = '/content/drive/My Drive/scaler.pkl'
METADATA_PATH = '/content/drive/My Drive/metadata.json'

# Load scaler
scaler = joblib.load(SCALER_PATH)

# Load metadata
with open(METADATA_PATH, "r") as f:
    metadata = json.load(f)

feature_order = metadata["feature_order"]
input_dim = metadata["input_dim"]


# --- Define model architecture (must match training model exactly) ---

class ConcreteRegressor(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        return self.net(x)

# Create model instance and load trained weights
model = ConcreteRegressor(input_dim=input_dim)
model.load_state_dict(torch.load(MODEL_PATH, map_location="cpu"))
model.eval()  # VERY IMPORTANT â€” ensure model is frozen

print("Model, scaler, and metadata loaded successfully.")

# Converts user-specified input values into a normalized tensor
# following the scaler and feature order from training.

def encode_inputs(input_dict, scaler, feature_order):
    """
    input_dict: {"Cement": 200, "Water": 150, ...}
    Returns a torch tensor shaped (1, input_dim)
    """

    # Ensure all required features are present
    values = [input_dict[col] for col in feature_order]

    # Convert to np.array and scale using training scaler
    X = np.array(values).reshape(1, -1)
    X_scaled = scaler.transform(X)

    # Convert to PyTorch tensor
    return torch.tensor(X_scaled, dtype=torch.float32)

# Custom loss that:
# 1. Minimizes difference from target output (regression)
# 2. Applies penalties for constraint violations
#
# This ensures optimization respects real-world limits.

def constraint_loss(pred, target, x, bounds=None, penalty_weight=10.0):
    """
    pred: model output
    target: desired output (concrete strength)
    x: the optimized input tensor
    bounds: {"Cement": (min, max), ...}
    """

    mse_term = (pred - target).pow(2).mean()

    if bounds is None:
        return mse_term  # no constraints

    penalty = 0

    # Apply constraint penalties feature-by-feature
    for i, col in enumerate(feature_order):
        min_val, max_val = bounds[col]

        # Penalty if out of range
        penalty += torch.relu(x[0, i] - max_val) ** 2
        penalty += torch.relu(min_val - x[0, i]) ** 2

    return mse_term + penalty_weight * penalty

# Performs gradient descent on the INPUT (x)
# rather than on the model weights.
# This generates the best input combination to achieve a desired output.

def optimize_inputs(
    model,
    scaler,
    feature_order,
    target_strength,
    bounds,
    steps=300,
    lr=0.01
):
    input_dim = len(feature_order)

    # Start with zeros in scaled space (requires_grad=True = optimize this tensor)
    x_opt = torch.zeros((1, input_dim), dtype=torch.float32, requires_grad=True)

    optimizer = optim.Adam([x_opt], lr=lr)

    for step in range(steps):
        optimizer.zero_grad()
        pred = model(x_opt)
        tgt = torch.tensor([[target_strength]], dtype=torch.float32)

        loss = constraint_loss(pred, tgt, x_opt, bounds=bounds)

        loss.backward()
        optimizer.step()

        # Optional: clamp inputs to [0, 1] to stay within normalized space
        x_opt.data = torch.clamp(x_opt.data, 0, 1)

        if step % 50 == 0:
            print(f"Step {step} | pred={pred.item():.3f} | loss={loss.item():.4f}")

    # Convert optimized scaled values back to original units
    x_np = x_opt.detach().numpy()
    x_unscaled = scaler.inverse_transform(x_np)[0]

    # Return dictionary of optimized ingredient values
    output_dict = {
        col: float(x_unscaled[i])
        for i, col in enumerate(feature_order)
    }

    return output_dict

# To demonstrate NIO, let's pick a target strength value.
# Your partner will replace this with API calls.

desired_strength = 50.0  # MPa (example target)

# Define realistic min/max bounds for each feature
# You or your partner can update these numbers.
bounds = {
    "Cement": (0, 540),
    "Blast Furnace Slag": (0, 360),
    "Fly Ash": (0, 200),
    "Water": (0, 250),
    "Superplasticizer": (0, 35),
    "Coarse Aggregate": (800, 1200),
    "Fine Aggregate": (600, 1000),
    "Age": (1, 365) # Added bounds for 'Age'
}

# Run NIO optimization
optimized_inputs = optimize_inputs(
    model=model,
    scaler=scaler,
    feature_order=feature_order,
    target_strength=desired_strength,
    bounds=bounds,
    steps=300,
    lr=0.01
)

optimized_inputs