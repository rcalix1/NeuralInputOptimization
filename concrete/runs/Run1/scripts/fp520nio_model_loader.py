# -*- coding: utf-8 -*-
"""FP520NIO Model Loader

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FjrqQgpuLr86mi2yVUIwy2prlJYjv1yI
"""

# model_loader.py
# This module loads:
#  - model_v1.pt        (trained PyTorch weights)
#  - scaler.pkl         (feature scaling)
#  - metadata.json      (feature order, input_dim, etc.)
#
# And exposes easy-to-use functions for FastAPI:
#  - load_artifacts()
#  - prepare_features()
#  - predict()

import torch
import torch.nn as nn
import json
import joblib
import numpy as np

# IMPORTANT:
# The model architecture MUST match the architecture used during training.
# FastAPI loads this model and uses the trained weights for inference.

class ConcreteRegressor(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        return self.net(x)

# Loads:
#   - scaler.pkl
#   - metadata.json
#   - model_v1.pt
# Returns:
#   model, scaler, metadata
# Mount Google Drive
from google.colab import drive
import os

drive.mount('/content/drive')

def load_artifacts(
    model_path='/content/drive/My Drive/model_v1.pt',
    scaler_path='/content/drive/My Drive/scaler.pkl',
    metadata_path='/content/drive/My Drive/metadata.json'
):
    # Load metadata (feature order + input_dim)
    with open(metadata_path, "r") as f:
        metadata = json.load(f)

    feature_order = metadata["feature_order"]
    input_dim = metadata["input_dim"]

    # Load scaler
    scaler = joblib.load(scaler_path)

    # Load model
    model = ConcreteRegressor(input_dim=input_dim)
    model.load_state_dict(torch.load(model_path, map_location="cpu"))
    model.eval()  # VERY IMPORTANT: keeps model frozen for inference

    return model, scaler, metadata

# This function converts input_dict from FastAPI into a scaled tensor.
#
# Example input_dict:
# {
#   "Cement": 200,
#   "Water": 150,
#   "Fly Ash": 35,
#   ...
# }
#
# FastAPI will pass request.json() directly into this function.

def prepare_features(input_dict, scaler, feature_order):
    # Extract feature values in the correct order
    values = [input_dict[col] for col in feature_order]

    # Convert to NumPy array and scale using the same scaler from training
    X = np.array(values).reshape(1, -1)
    X_scaled = scaler.transform(X)

    # Convert to tensor for PyTorch inference
    return torch.tensor(X_scaled, dtype=torch.float32)

# This function is what your FastAPI endpoint will call.
# It makes the prediction by:
#   1. Preparing input features
#   2. Passing them through the trained model
#   3. Returning the predicted concrete strength

def predict(model, scaler, metadata, input_dict):
    # Extract feature order
    feature_order = metadata["feature_order"]

    # Convert inputs into a PyTorch tensor
    X_tensor = prepare_features(input_dict, scaler, feature_order)

    # Run inference
    with torch.no_grad():
        pred = model(X_tensor).item()

    # Return the predicted concrete compressive strength
    return pred

# This cell shows you how to test the loader.

# Load artifacts
model, scaler, metadata = load_artifacts()

# Example input
example = {
    "Cement": 200,
    "Blast Furnace Slag": 50,
    "Fly Ash": 30,
    "Water": 150,
    "Superplasticizer": 10,
    "Coarse Aggregate": 1000,
    "Fine Aggregate": 800,
    "Age": 28
}

# Run prediction
predicted_strength = predict(model, scaler, metadata, example)
predicted_strength